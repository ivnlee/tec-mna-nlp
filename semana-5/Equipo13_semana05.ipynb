{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivnlee/tec-mna-nlp/blob/main/semana-5/Equipo13_semana05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSwWkvNfdMcy"
      },
      "source": [
        "## Continuando con caso de estudio: Amazon-Yelp-Imdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCOg5pHpdOoM",
        "outputId": "998889f7-ff33-42b5-fb8a-6c5c0a3b5963"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS28nxS6fjRr",
        "outputId": "c57bb53d-97f8-4138-98ec-90b4ffca1856"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42HNEa-Tfvyj",
        "outputId": "2cecadf5-0542-4557-9bf3-6ff0fa5632ab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lpjqaIXbdMc2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  \n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk.download('omw-1.4')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhhKFMdGd2-5",
        "outputId": "8c44b275-ba62-4cff-801b-91b7e311403c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNUMEHVfd5Qn",
        "outputId": "2529b450-2b25-4482-e80c-9c5d9dae514c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFjSzuMbdMc3"
      },
      "source": [
        "### **1. Cargar datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sjML9yBdMc4",
        "outputId": "086db121-ae41-47b7-e6ef-e5ee6cdac481"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de registros Amazon:  (1000, 2)\n",
            "Total de registros IMDB:  (1000, 2)\n",
            "Total de registros Yelp:  (1000, 2)\n"
          ]
        }
      ],
      "source": [
        "# Cargamos los datos, observamos que para el archivo de IMDB \n",
        "# el separador es de 3 o más espacios y no un tabulador:\n",
        "\n",
        "dfa = pd.read_csv('/content/drive/MyDrive/NLP/semana-5/amazon5.txt', sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
        "dfi = pd.read_csv('/content/drive/MyDrive/NLP/semana-5/imdb5.txt', sep=r'\\s{3,}', names=['review','label'], header=None, encoding='utf-8', engine='python')\n",
        "dfy = pd.read_csv('/content/drive/MyDrive/NLP/semana-5/yelp5.txt', sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
        "\n",
        "# verificamos total de registros\n",
        "print('Total de registros Amazon: ', dfa.shape)\n",
        "print('Total de registros IMDB: ', dfi.shape)\n",
        "print('Total de registros Yelp: ', dfy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPRfcXO_dMc5",
        "outputId": "6ecc3b0f-c5e5-4286-a0b9-a0f9b4a4874c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   review  3000 non-null   object\n",
            " 1   label   3000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 47.0+ KB\n"
          ]
        }
      ],
      "source": [
        "# unimos los 3 dataframes\n",
        "df = pd.concat([dfa, dfi, dfy], ignore_index=True)\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPVJ6V57dMc6"
      },
      "source": [
        "### **2. Limpieza y lematización**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s-KvaPW7dMc6"
      },
      "outputs": [],
      "source": [
        "def clean_tok(doc):\n",
        "  # Elimina los signos de puntuación, caracteres especiales y números\n",
        "  tokens = re.sub(r'[^A-Za-z\\s]', ' ', doc).strip()\n",
        "\n",
        "  # Elimina espacios en blanco adicionales y convierte a minúsculas\n",
        "  tokens = ' '.join(re.findall(r'\\b\\w+\\b', tokens.lower()))\n",
        "  \n",
        "  # Tokeniza por palabras, elimina stopwords y palabras de longitud menor a 1\n",
        "  tokens = [w for w in tokens.split() if w not in stopwords.words('english') and len(w) > 1]\n",
        "\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kHFH0708dMc7"
      },
      "outputs": [],
      "source": [
        "# Separamos las palabras de las etiquetas\n",
        "\n",
        "X = df.review \n",
        "Y = df.label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6xSm1AEcdMc7"
      },
      "outputs": [],
      "source": [
        "# aplicamos la función clean_tok a cada registro de X\n",
        "\n",
        "Xcleantok = [clean_tok(x) for x in X] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUVKL_vedMc8"
      },
      "source": [
        "Obsrvamos los primeros registros "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2On08kddMc8",
        "outputId": "1b0e8593-9266-4536-9dd0-6fce59f08e3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['way', 'plug', 'us', 'unless', 'go', 'converter']\n",
            "['good', 'case', 'excellent', 'value']\n",
            "['great', 'jawbone']\n",
            "['tied', 'charger', 'conversations', 'lasting', 'minutes', 'major', 'problems']\n",
            "['mic', 'great']\n",
            "['jiggle', 'plug', 'get', 'line', 'right', 'get', 'decent', 'volume']\n",
            "['several', 'dozen', 'several', 'hundred', 'contacts', 'imagine', 'fun', 'sending', 'one', 'one']\n",
            "['razr', 'owner', 'must']\n",
            "['needless', 'say', 'wasted', 'money']\n",
            "['waste', 'money', 'time']\n"
          ]
        }
      ],
      "source": [
        "for x in Xcleantok[0:10]:\n",
        "  print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO8o6VrkdMc8"
      },
      "source": [
        "### Lematización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0Wu7fU9sdMc9"
      },
      "outputs": [],
      "source": [
        "wnl = WordNetLemmatizer() \n",
        "\n",
        "def lemmatizer(doc):\n",
        "\n",
        "  Xtmp = [wnl.lemmatize(w, 'v') for w in doc]     # lemmatiza verbos\n",
        "  Xtmp = [wnl.lemmatize(w, 'a') for w in Xtmp]    # lemmatiza adjetivos\n",
        "  Xtmp = [wnl.lemmatize(w, 'n') for w in Xtmp]    # lemmatiza sustantivos\n",
        "  Xtmp = [wnl.lemmatize(w, 'r') for w in Xtmp]    # lemmatiza adverbios\n",
        "  \n",
        "  return Xtmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RDeh6aiPdMc9"
      },
      "outputs": [],
      "source": [
        "# aplicamos la función lemmatizer a cada registro de Xcleantok\n",
        "\n",
        "Xclean = [lemmatizer(x) for x in Xcleantok]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfMpxTa0dMc9",
        "outputId": "a386174f-1684-4b5d-e794-bd9629239ee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['way', 'plug', 'u', 'unless', 'go', 'converter']\n",
            "['good', 'case', 'excellent', 'value']\n",
            "['great', 'jawbone']\n",
            "['tie', 'charger', 'conversation', 'last', 'minute', 'major', 'problem']\n",
            "['mic', 'great']\n",
            "['jiggle', 'plug', 'get', 'line', 'right', 'get', 'decent', 'volume']\n",
            "['several', 'dozen', 'several', 'hundred', 'contact', 'imagine', 'fun', 'send', 'one', 'one']\n",
            "['razr', 'owner', 'must']\n",
            "['needle', 'say', 'waste', 'money']\n",
            "['waste', 'money', 'time']\n"
          ]
        }
      ],
      "source": [
        "for x in Xclean[0:10]:\n",
        "    print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm7MrP74dMc9"
      },
      "source": [
        "### **3. Train-Validation-Test split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHaqfXETdMc-",
        "outputId": "b8cc8260-10fe-4037-dd9c-68ab07300539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X,y Train: 2100 2100\n",
            "X,y Val: 450 450\n",
            "X,y Test 450 450\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "x_train, x_val_test, y_train, y_val_test = train_test_split(Xclean, Y, test_size=0.30, shuffle=True, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_val_test, y_val_test, test_size=0.50, shuffle=True, random_state=42) \n",
        "\n",
        "# Verificamos el tamaño de los conjuntos de datos de entrenamiento, validación y prueba\n",
        "print('X,y Train:', len(x_train), len(y_train)) \n",
        "print('X,y Val:', len(x_val), len(y_val))\n",
        "print('X,y Test', len(x_test), len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Vocabulario**"
      ],
      "metadata": {
        "id": "NQs7FMFqp6Ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "taLH8Y6kp_Ok"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diccionario = Counter()\n",
        "\n",
        "# creamos diccionario con las palabras del set de entrenamiento\n",
        "for k in range(len(x_train)):\n",
        "  diccionario.update(x_train[k]) \n",
        "\n",
        "print('Tamaño del diccionario: ', len(diccionario))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyZOj7LmqCuU",
        "outputId": "c02b428a-fe6d-4044-8932-cb464a484e92"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del diccionario:  3253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filtramos el diccionario\n",
        "frec_min = 1\n",
        "\n",
        "dicc = {k:v for k,v in diccionario.items() if v > frec_min}\n",
        "\n",
        "print('Tamaño del diccionario filtrado: ', len(dicc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AVk5ErzqGod",
        "outputId": "b22a90aa-c23e-492a-fe90-f3b9748caf92"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del diccionario filtrado:  1429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtramos los conjuntos de datos de entrenamiento, validación y prueba"
      ],
      "metadata": {
        "id": "6mThtWuxqMMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = []\n",
        "for n in x_train:\n",
        "    train_x.append([w for w in n if w in dicc])\n",
        "\n",
        "val_x = []\n",
        "for n in x_val:\n",
        "    val_x.append([w for w in n if w in dicc])\n",
        "\n",
        "test_x = []\n",
        "for n in x_test:\n",
        "    test_x.append([w for w in n if w in dicc])"
      ],
      "metadata": {
        "id": "cYWN7xqxqMsQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Tabla comparativa de pros y contras entre los modelos FastText, word2vec de Google y Glove de Stanford.**"
      ],
      "metadata": {
        "id": "PMjVAuv_qh3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "UkTFqYC2qxwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Diccionario con Vectores Embebidos de FastText**"
      ],
      "metadata": {
        "id": "Dftp7095q1Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install fasttext"
      ],
      "metadata": {
        "id": "5FrW209GrLKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import fasttext.util"
      ],
      "metadata": {
        "id": "wIna_OACsLcU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fasttext.util.download_model('en', if_exists='ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "CeiqTyMLsVUc",
        "outputId": "7e83d2ee-d2b2-4810-eed3-939380444bc5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cc.en.300.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft = fasttext.load_model('cc.en.300.bin')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtAyZrLr7Uo9",
        "outputId": "b0b2be52-4ccd-44c8-f545-0cf2565e8c97"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(ft.words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LMpiNhW8qUH",
        "outputId": "d264ea81-ce7d-4ac0-a256-6f95c2f0d2a8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000000"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft.get_dimension()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxpYb0a38uZn",
        "outputId": "73039985-ac0c-4fb3-a1ef-fbd339db12f0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "midicc = {}\n",
        "\n",
        "# extraemos los vectores mebebidos para nuestro diccionario\n",
        "for k,w in enumerate(dicc.keys()):\n",
        "  tmp = ft.get_word_vector(w)\n",
        "  midicc.update({w:tmp})\n"
      ],
      "metadata": {
        "id": "xm-sXpuqATYO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verificamos que el nuevo diccionario con vectores embebidos sea del mismo tamaño de nuestro vocabulario:"
      ],
      "metadata": {
        "id": "pdAPgRW5E9ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dicc) == len(midicc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djNw-trZFLAu",
        "outputId": "a35f5fe8-acb1-4b42-84b8-da556af715a2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guardamos el diccionario en un archivo pickle"
      ],
      "metadata": {
        "id": "mmDZLS5XHFsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "#with open('midicc_emb.pickle', 'wb') as handle:\n",
        "#    pickle.dump(midicc, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "### para cargar nuevamente el archivo usamos el siguiente codigo: ###\n",
        "#with open('midicc_emb.pickle', 'rb') as handle:\n",
        "#    midicc = pickle.load(handle)"
      ],
      "metadata": {
        "id": "fUStOEClHKmt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# borramos la variable ft para liberar memoria RAM\n",
        "del ft "
      ],
      "metadata": {
        "id": "9VCEaW8qL5cc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Asignar a cada comentario filtrado el vector embebido de dimensión 300**"
      ],
      "metadata": {
        "id": "9mychCbfLNxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding_vector(docs):\n",
        "    dim = 300\n",
        "    zero_vector = np.zeros(dim)\n",
        "    vectors = []\n",
        "    for tokens in docs:\n",
        "        tmp_vect =  np.zeros(dim)\n",
        "        tmp_count = 0 + 1e-5 # para evitar dividir por 0\n",
        "        for token in tokens:\n",
        "            if token in midicc:\n",
        "                tmp_vect += midicc[token]\n",
        "                tmp_count +=1\n",
        "        if(tmp_count!=0):\n",
        "            vectors.append(tmp_vect/tmp_count) \n",
        "        else:\n",
        "            vectors.append(zero_vector)\n",
        "    return vectors"
      ],
      "metadata": {
        "id": "Jwh8KeEdLkP0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainEmb = embedding_vector(train_x) \n",
        "\n",
        "valEmb = embedding_vector(val_x)\n",
        "\n",
        "testEmb = embedding_vector(test_x) "
      ],
      "metadata": {
        "id": "sMMxVhQBZ2AM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('X Train: ', len(trainEmb))\n",
        "print('X Val: ', len(valEmb))\n",
        "print('X Test: ', len(testEmb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3JGkZ7TaaJU",
        "outputId": "4d59d847-d100-44fb-a410-cec3431cba8f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X Train:  2100\n",
            "X Val:  450\n",
            "X Test:  450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. Modelos de Regresión Logística y Random Forest**"
      ],
      "metadata": {
        "id": "2fRVMNuZeNf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "KhdAjYwJeVG9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modeloLRemb = LogisticRegression(max_iter=50,\n",
        "                                   C=1.0,\n",
        "                                   solver='liblinear',\n",
        "                                   random_state=42)\n",
        "\n",
        "modeloLRemb.fit(trainEmb, y_train)\n",
        "\n",
        "\n",
        "modeloRFemb = RandomForestClassifier(n_estimators=10,\n",
        "                                       max_depth=4,\n",
        "                                       random_state=42)\n",
        "\n",
        "modeloRFemb.fit(trainEmb, y_train)\n",
        "\n",
        "########################################################################################\n",
        "\n",
        "print('LR: Train-accuracy: %.2f%%' % (100*modeloLRemb.score(trainEmb, y_train)))\n",
        "print('LR: Val-accuracy: %2.f%%' % (100*modeloLRemb.score(valEmb, y_val)))\n",
        "\n",
        "print('\\nRF: Train-accuracy: %.2f%%' % (100*modeloRFemb.score(trainEmb, y_train)))\n",
        "print('RF: Val-accuracy: %.2f%%' % (100*modeloRFemb.score(valEmb, y_val)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao0A-RU5eal_",
        "outputId": "6039ad49-f938-49dc-ce31-83646388fa8e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR: Train-accuracy: 80.57%\n",
            "LR: Val-accuracy: 76%\n",
            "\n",
            "RF: Train-accuracy: 80.10%\n",
            "RF: Val-accuracy: 72.22%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}