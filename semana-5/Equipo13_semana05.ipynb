{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuando con caso de estudio: Amazon-Yelp-Imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Cargar datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros Amazon:  (1000, 2)\n",
      "Total de registros IMDB:  (1000, 2)\n",
      "Total de registros Yelp:  (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Cargamos los datos, observamos que para el archivo de IMDB \n",
    "# el separador es de 3 o más espacios y no un tabulador:\n",
    "\n",
    "dfa = pd.read_csv('amazon5.txt', sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
    "dfi = pd.read_csv('imdb5.txt', sep=r'\\s{3,}', names=['review','label'], header=None, encoding='utf-8', engine='python')\n",
    "dfy = pd.read_csv('yelp5.txt', sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
    "\n",
    "# verificamos total de registros\n",
    "print('Total de registros Amazon: ', dfa.shape)\n",
    "print('Total de registros IMDB: ', dfi.shape)\n",
    "print('Total de registros Yelp: ', dfy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  3000 non-null   object\n",
      " 1   label   3000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 47.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# unimos los 3 dataframes\n",
    "df = pd.concat([dfa, dfi, dfy], ignore_index=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Limpieza y lematización**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tok(doc):\n",
    "  # Elimina los signos de puntuación, caracteres especiales y números\n",
    "  tokens = re.sub(r'[^A-Za-z\\s]', ' ', doc).strip()\n",
    "\n",
    "  # Elimina espacios en blanco adicionales y convierte a minúsculas\n",
    "  tokens = ' '.join(re.findall(r'\\b\\w+\\b', tokens.lower()))\n",
    "  \n",
    "  # Tokeniza por palabras, elimina stopwords y palabras de longitud menor a 1\n",
    "  tokens = [w for w in tokens.split() if w not in stopwords.words('english') and len(w) > 1]\n",
    "\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos las palabras de las etiquetas\n",
    "\n",
    "X = df.review \n",
    "Y = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicamos la función clean_tok a cada registro de X\n",
    "\n",
    "Xcleantok = [clean_tok(x) for x in X] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obsrvamos los primeros registros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['way', 'plug', 'us', 'unless', 'go', 'converter']\n",
      "['good', 'case', 'excellent', 'value']\n",
      "['great', 'jawbone']\n",
      "['tied', 'charger', 'conversations', 'lasting', 'minutes', 'major', 'problems']\n",
      "['mic', 'great']\n",
      "['jiggle', 'plug', 'get', 'line', 'right', 'get', 'decent', 'volume']\n",
      "['several', 'dozen', 'several', 'hundred', 'contacts', 'imagine', 'fun', 'sending', 'one', 'one']\n",
      "['razr', 'owner', 'must']\n",
      "['needless', 'say', 'wasted', 'money']\n",
      "['waste', 'money', 'time']\n"
     ]
    }
   ],
   "source": [
    "for x in Xcleantok[0:10]:\n",
    "  print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer() \n",
    "\n",
    "def lemmatizer(doc):\n",
    "\n",
    "  Xtmp = [wnl.lemmatize(w, 'v') for w in doc]     # lemmatiza verbos\n",
    "  Xtmp = [wnl.lemmatize(w, 'a') for w in Xtmp]    # lemmatiza adjetivos\n",
    "  Xtmp = [wnl.lemmatize(w, 'n') for w in Xtmp]    # lemmatiza sustantivos\n",
    "  Xtmp = [wnl.lemmatize(w, 'r') for w in Xtmp]    # lemmatiza adverbios\n",
    "  \n",
    "  return Xtmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicamos la función lemmatizer a cada registro de Xcleantok\n",
    "\n",
    "Xclean = [lemmatizer(x) for x in Xcleantok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['way', 'plug', 'u', 'unless', 'go', 'converter']\n",
      "['good', 'case', 'excellent', 'value']\n",
      "['great', 'jawbone']\n",
      "['tie', 'charger', 'conversation', 'last', 'minute', 'major', 'problem']\n",
      "['mic', 'great']\n",
      "['jiggle', 'plug', 'get', 'line', 'right', 'get', 'decent', 'volume']\n",
      "['several', 'dozen', 'several', 'hundred', 'contact', 'imagine', 'fun', 'send', 'one', 'one']\n",
      "['razr', 'owner', 'must']\n",
      "['needle', 'say', 'waste', 'money']\n",
      "['waste', 'money', 'time']\n"
     ]
    }
   ],
   "source": [
    "for x in Xclean[0:10]:\n",
    "    print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Train-Validation-Test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X,y Train: 2100 2100\n",
      "X,y Val: 450 450\n",
      "X,y Test 450 450\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_val_test, y_train, y_val_test = train_test_split(Xclean, Y, test_size=0.30, shuffle=True, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val_test, y_val_test, test_size=0.50, shuffle=True, random_state=42) \n",
    "\n",
    "# Verificamos el tamaño de los conjuntos de datos de entrenamiento, validación y prueba\n",
    "print('X,y Train:', len(x_train), len(y_train)) \n",
    "print('X,y Val:', len(x_val), len(y_val))\n",
    "print('X,y Test', len(x_test), len(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
